{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebookContains one driving code that combines both the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import imutils\n",
    "import sys\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['haarcascade_eye.xml',\n",
       " '.ipynb_checkpoints',\n",
       " 'README.md',\n",
       " 'example.mp4',\n",
       " 'Human_activity_Recog.py',\n",
       " '.gitattributes',\n",
       " 'example_activities.mp4',\n",
       " 'HAR+FaceRecognition.ipynb',\n",
       " 'resnet-34_kinetics.onnx',\n",
       " 'action_recognition_kinetics.txt',\n",
       " 'face_recognition_commented.py',\n",
       " 'haarcascade_frontalface_default.xml',\n",
       " '.git']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd=os.listdir()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"resnet-34_kinetics.onnx\"\n",
    "CLASSES=open(\"action_recognition_kinetics.txt\").read().strip().split(\"\\n\")\n",
    "input_video = \"example.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') # We load the cascade for the face.\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml') # We load the cascade for the eyes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DURATION = 16  # duration (i.e., # of frames for classification)\n",
    "SAMPLE_SIZE = 112     # and sample size (i.e., the spatial dimensions of the frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(input_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(gray, frame): # We create a function that takes as input the image in black and white (gray) and the original image (frame), and that will return the same image with the detector rectangles.\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5) # We apply the detectMultiScale method from the face cascade to locate one or several faces in the image.\n",
    "    for (x, y, w, h) in faces: # For each detected face:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2) # We paint a rectangle around the face.\n",
    "        roi_gray = gray[y:y+h, x:x+w] # We get the region of interest in the black and white image.\n",
    "        roi_color = frame[y:y+h, x:x+w] # We get the region of interest in the colored image.\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, 1.1, 3) # We apply the detectMultiScale method to locate one or several eyes in the image.\n",
    "        for (ex, ey, ew, eh) in eyes: # For each detected eye:\n",
    "            cv2.rectangle(roi_color,(ex, ey),(ex+ew, ey+eh), (0, 255, 0), 2) # We paint a rectangle around the eyes, but inside the referential of the face.\n",
    "    return frame # We return the image with the detector rectangles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "yoga\n",
      "playing violin\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "playing violin\n",
      "playing violin\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "yoga\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n",
      "playing violin\n"
     ]
    }
   ],
   "source": [
    "while True: # We repeat infinitely (until break):\n",
    "    frames = []\n",
    "    _, frame = video_capture.read() # We get the last frame.\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    frames.append(frame)\n",
    "\n",
    "    # now that our frames array is filled we can construct our blob\n",
    "    blob = cv2.dnn.blobFromImages(frames, 1.0,(SAMPLE_SIZE, SAMPLE_SIZE), (114.7748, 107.7354, 99.4750),swapRB=True, crop=True)\n",
    "    blob = np.transpose(blob, (1, 0, 2, 3))\n",
    "    blob = np.expand_dims(blob, axis=0)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    outputs = net.forward()\n",
    "    label = CLASSES[np.argmax(outputs)]\n",
    "\n",
    "    print(label)\n",
    "    cv2.putText(frame, label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX,0.8, (255, 255, 255), 2)\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # We do some colour transformations.\n",
    "    canvas = detect(gray, frame) # We get the output of our detect function.\n",
    "\n",
    "    cv2.imshow('Video', canvas) # We display the outputs.\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # If we type on the keyboard:\n",
    "        break # We stop the loop.\n",
    "\n",
    "video_capture.release() # We turn the webcam off.\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-07019295ca77>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-07019295ca77>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    blob = cv2.dnn.blobFromImages(frames, 1.0,SAMPLE_SIZE, SAMPLE_SIZE), (114.7748, 107.7354, 99.4750),swapRB=True, crop=True)\u001b[0m\n\u001b[0m                                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# loop until we explicitly break from it\n",
    "while True:\n",
    "    # initialize the batch of frames that will be passed through the\n",
    "    # model\n",
    "    frames = []\n",
    "\n",
    "    # loop over the number of required sample frames\n",
    "    for i in range(0, SAMPLE_DURATION):\n",
    "        # read a frame from the video stream\n",
    "        (grabbed, frame) = vs.read()\n",
    "\n",
    "        # if the frame was not grabbed then we've reached the end of\n",
    "        # the video stream so exit the script\n",
    "        if not grabbed:\n",
    "            print(\"[INFO] no frame read from stream - exiting\")\n",
    "            sys.exit(0)\n",
    "\n",
    "        # otherwise, the frame was read so resize it and add it to\n",
    "        # our frames list\n",
    "        frame = imutils.resize(frame, width=400)\n",
    "        frames.append(frame)\n",
    "\n",
    "    # now that our frames array is filled we can construct our blob\n",
    "    blob = cv2.dnn.blobFromImages(frames, 1.0,(SAMPLE_SIZE, SAMPLE_SIZE), (114.7748, 107.7354, 99.4750),\n",
    "\t\tswapRB=True, crop=True)\n",
    "\tblob = np.transpose(blob, (1, 0, 2, 3))\n",
    "\tblob = np.expand_dims(blob, axis=0)\n",
    "\n",
    "\t# pass the blob through the network to obtain our human activity\n",
    "\t# recognition predictions\n",
    "\tnet.setInput(blob)\n",
    "\toutputs = net.forward()\n",
    "\tlabel = CLASSES[np.argmax(outputs)]\n",
    "\n",
    "\t# loop over our frames\n",
    "\tfor frame in frames:\n",
    "\t\t# draw the predicted activity on the frame\n",
    "\t\tcv2.rectangle(frame, (0, 0), (300, 40), (0, 0, 0), -1)\n",
    "\t\tcv2.putText(frame, label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX,0.8, (255, 255, 255), 2)\n",
    "\n",
    "\t\t# display the frame to our screen\n",
    "\t\tcv2.imshow(\"Activity Recognition\", frame)\n",
    "\t\tkey = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\t\t# if the `q` key was pressed, break from the loop\n",
    "\t\tif key == ord(\"q\"):\n",
    "\t\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
